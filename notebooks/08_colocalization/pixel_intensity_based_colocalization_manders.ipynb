{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e296f4b",
   "metadata": {},
   "source": [
    "# Mander's Correlation Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /// script\n",
    "# requires-python = \">=3.12\"\n",
    "# dependencies = [\n",
    "#     \"matplotlib\",\n",
    "#     \"ndv[jupyter,vispy]\",\n",
    "#     \"scikit-image\",\n",
    "#     \"numpy\",\n",
    "#     \"scipy\",\n",
    "#     \"tifffile\",\n",
    "#     \"imagecodecs\",\n",
    "#     \"tqdm\",\n",
    "# ]\n",
    "# ///"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc72aa",
   "metadata": {},
   "source": [
    "## <mark style=\"color: black; background-color: rgb(127,196,125); padding: 3px; border-radius: 5px;\">Description</mark>\n",
    "\n",
    "In this notebook, we will explore how to implement **Mander's Correlation Coefficients** in Python, a common method for quantifying colocalization based on pixel intensities.\n",
    "\n",
    "The images we will use for this section can be downloaded from the <a href=\"../../../_static/data/08_pixel_intensity_based_coloc.zip\" download> <i class=\"fas fa-download\"></i> Mander's & Pearson's Colocalization Dataset</a>.\n",
    "\n",
    "<p class=\"alert alert-warning\">\n",
    "   <strong>Note:</strong> This notebook aims to show how to practically implement these methods but does not aim to describe when to use this method. The images used have been selected to showcase the practical implementation of the methods.\n",
    "</p>\n",
    "\n",
    "<p class=\"alert alert-info\">\n",
    "    <strong>Note:</strong> In this example, we will not perform any image processing steps before computing the Mander's Correlation Coefficients. However, when conducting a real colocalization analysis, you should consider applying some image processing steps to clean the images before computing the Mander's Correlation Coefficients, such as background subtraction, flat-field correction, etc.\n",
    "</p>\n",
    "\n",
    "<p class=\"alert alert-info\">\n",
    "    <strong>Note:</strong> In this notebook, we will only use a single image pair for demonstration purposes. Often, Mander's coefficients should not be interpreted as absolute values in isolation. Instead, it's always recommended to consider them in the context of comparisons between different conditions, controls, treatments, or experimental groups. The relative changes and ratios between conditions are often more meaningful than the absolute coefficient values themselves.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1dd28d",
   "metadata": {},
   "source": [
    "## <mark style=\"color: black; background-color: rgb(127,196,125); padding: 3px; border-radius: 5px;\">Mander's Correlation Coefficients</mark>\n",
    "\n",
    "Mander's correlation coefficients can be used to quantify the degree of colocalization between two channels (or images). These coefficients, M1 and M2, are calculated based on the pixel intensities of the two channels and for this reason, they are different from a simple area overlap.\n",
    "\n",
    "**M1** measures the **fraction of channel 1 intensity that co-occurs with channel 2**:\n",
    "- **Numerator**: Sum of channel 1 intensities in pixels where both channels are above their thresholds\n",
    "- **Denominator**: Sum of all channel 1 intensities above threshold\n",
    "\n",
    "**M2** measures the **fraction of channel 2 intensity that co-occurs with channel 1**:\n",
    "- **Numerator**: Sum of channel 2 intensities in pixels where both channels are above their thresholds\n",
    "- **Denominator**: Sum of all channel 2 intensities above threshold\n",
    "\n",
    "<div> <img src=\"https://raw.githubusercontent.com/HMS-IAC/bobiac/main/_static/images/coloc/manders_slide.png\" alt=\"manders\" width=\"800\"></div>\n",
    "\n",
    "For this exercise, we will analyze an image of a HeLa cell stained with two fluorescent markers: **channel 1** labels **endosomes** and **channel 2** labels **lysosomes** (<a href=\"../../_static/data/08_pixel_intensity_based_coloc.zip\" download><i class=\"fas fa-download\"></i>Mander's & Pearson's Colocalization Dataset</a>.)\n",
    "\n",
    "From a biological perspective, lysosomes are typically found within or closely associated with endosomal compartments, while endosomes have a broader cellular distribution. Based on this biology, we expect:\n",
    "\n",
    "- **High M2 coefficient**: Most lysosomal signal should colocalize with endosomal regions\n",
    "- **Lower M1 coefficient**: Only a subset of endosomal signal should colocalize with lysosomes, since endosomes are more widely distributed throughout the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f616961",
   "metadata": {},
   "source": [
    "### <mark style=\"color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;\">Import Libraries</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f724dea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ef6900a",
   "metadata": {},
   "source": [
    "### <mark style=\"color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;\">Load and Visualize the Image</mark>\n",
    "\n",
    "Open and visualize (with ndv) the image named `cells_manders_14na.tif` from the <a href=\"../../_static/data/08_pixel_intensity_based_coloc.zip\" download><i class=\"fas fa-download\"></i> Mander's & Pearson's Colocalization Dataset</a>. This is a two-channel image where channel 1 has stained endosomes and channel 2 has stained lysosomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58157086",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca3d02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98764c14",
   "metadata": {},
   "source": [
    "To compute Mander's Correlation Coefficients, we need **two separate images** (channels). \n",
    "\n",
    "What is the image shape? How do we split the channels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86327fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9bf352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7bc303e",
   "metadata": {},
   "source": [
    "### <mark style=\"color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;\">Calculate Numerators and Denominators for Mander's Correlation Coefficients</mark>\n",
    "\n",
    "The first and key step is to calculate **$R_i^{}$** and **$G_i^{}$** and thus to select which areas of each channel we want to consider for the colocalization analysis. This means we first need to **threshold each images to select only the pixels we want to consider**.\n",
    "\n",
    "It is therefore evident that Mander's Correlation Coefficients are **sensitive to thresholding**, the way you decide to threshold your images will have a large impact on the results.\n",
    "\n",
    "For this example, we will first use a simple Otsu thresholding method and later in the notebook we will explore a more automated way of selecting the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d3aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0642f375",
   "metadata": {},
   "source": [
    "We can plot the raw data and the masks in a 2x2 subplot to visualize the results of the thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526f6cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fce32fb",
   "metadata": {},
   "source": [
    "Now that we have the mask for each channel, we can first **calculate the overlap mask** where both channels are above their respective thresholds, and then calculate **$R_i^{coloc}$** and **$G_i^{coloc}$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81202e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a3076eb",
   "metadata": {},
   "source": [
    "With the overlap mask, we can now calculate the **$R_i^{coloc}$** (*ch1_coloc*) and **$G_i^{coloc}$** (*ch2_coloc*) and the **numerator** for the Mander's Correlation Coefficients: **sum($R_i^{coloc}$)** and **sum($G_i^{coloc}$)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103976dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ce9ff23",
   "metadata": {},
   "source": [
    "We can now **calculate the denominator** for the Manders coefficients.\n",
    "<br>\n",
    "The denominator is the sum of the pixel intensities in the overlap mask for each channel above their respective thresholds: **sum($R_i^{}$)** and **sum($G_i^{}$)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085491bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9eaf3fb",
   "metadata": {},
   "source": [
    "### <mark style=\"color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;\">Calculate Mander's Correlation Coefficients</mark>\n",
    "\n",
    "Now with both numerators and denominators calculated, we can compute the Manders coefficients M1 and M2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ccf24f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "792b68b8",
   "metadata": {},
   "source": [
    "With Otsu thresholding for both channels, we obtain:\n",
    "\n",
    "**M1=0.3496** and **M2=0.8975**\n",
    "\n",
    "- **M1** indicates that approximately **35%** (0.3496) of channel 1's intensity colocalizes with channel 2. This means that about one-third of channel 1's signal overlaps with areas where channel 2 is also present above threshold.\n",
    "\n",
    "- **M2** indicates that approximately **90%** (0.8975) of channel 2's intensity colocalizes with channel 1. This suggests that nearly all of channel 2's signal overlaps with areas where channel 1 is also present above threshold.\n",
    "\n",
    "This asymmetry (M1 ≠ M2) is common and tells us that **channel 2 is largely contained within areas where channel 1 is present**, but **channel 1 extends beyond the regions where channel 2 is found**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12baf806",
   "metadata": {},
   "source": [
    "**Bonus**: In the <a href=\"../../_static/data/08_pixel_intensity_based_coloc.zip\" download> <i class=\"fas fa-download\"></i> Mander's & Pearson's Colocalization Dataset</a> there is an image named `cells_manders_0.3na.tif`, the exact same image we just used nut acquired with a smaller numerical aperture (NA) of the objective lens.\n",
    "\n",
    "What do you think will happen to the Mander's coefficients if we use this image instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fedfc5",
   "metadata": {},
   "source": [
    "### <mark style=\"color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;\">Costes Auto-Threshold Method</mark>\n",
    "\n",
    "As mentioned above, the Mender's Correlation Coefficients are sensitive to thresholding, so the way you decide to threshold your images will have a large impact on the results.\n",
    "\n",
    "The function `costes_auto_threshold` below implements the [Costes auto-threshold method](https://pmc.ncbi.nlm.nih.gov/articles/PMC1304300/), which **automatically determines optimal threshold values for both channels**. \n",
    "\n",
    "The method works by finding threshold values where pixels *below* these thresholds show no statistical correlation (Pearson correlation coefficient ≈ 0). This approach helps objectively separate true signal from background noise.\n",
    "\n",
    "The algorithm performs orthogonal linear regression between the two channels to establish their relationship, then iteratively tests threshold pairs derived from this regression to identify the optimal separation point between signal and background.\n",
    "\n",
    "Of course, the Costes auto-threshold method has limitations and may not work in certain scenarios, including:\n",
    "- **Insufficient data**: When there are too few non-zero pixels (< 10) in either channel\n",
    "- **No linear relationship**: When channels show non-linear, multiple population, or no correlation patterns.\n",
    "- **Low variance**: When one or both channels have uniform or near-uniform intensities\n",
    "- **High background noise**: When noise dominates the signal relationship\n",
    "- **Limited dynamic range**: Narrow intensity ranges or saturated pixels\n",
    "\n",
    "In such cases, alternative thresholding methods (Otsu, manual, percentile-based) may be more appropriate.\n",
    "\n",
    "We can now try to compute and print the Mander's Correlation Coefficients using the Costes auto-threshold method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04775d7",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def costes_auto_threshold(\n",
    "    ch1: np.ndarray,\n",
    "    ch2: np.ndarray,\n",
    "    num_thresholds: int = 100,\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Implementation of Costes auto-threshold method for colocalization analysis.\n",
    "\n",
    "    Based on:\n",
    "    Costes et al. \"Automatic and quantitative measurement of protein-protein\n",
    "    colocalization in live cells\" Biophysical Journal 2004\n",
    "    https://pmc.ncbi.nlm.nih.gov/articles/PMC1304300/\n",
    "\n",
    "    The method finds thresholds where the Pearson correlation coefficient\n",
    "    of pixels below the thresholds equals zero, indicating that pixels\n",
    "    below these thresholds show no statistical correlation.\n",
    "\n",
    "    This implementation ensures symmetric results regardless of channel order.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    ch1: np.ndarray\n",
    "        First channel image data (2D array).\n",
    "    ch2: np.ndarray\n",
    "        Second channel image data (2D array).\n",
    "    num_thresholds: int\n",
    "        Number of threshold values to test along the regression line. By default, 100.\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    tuple: (threshold_ch1, threshold_ch2, slope, intercept)\n",
    "        Optimal thresholds for channel 1 and channel 2, slope and intercept of the\n",
    "        regression line that relates ch2 to ch1 (ch2 = slope * ch1 + intercept).\n",
    "    \"\"\"\n",
    "\n",
    "    # Flatten images for easier processing\n",
    "    ch1_flat = ch1.ravel()\n",
    "    ch2_flat = ch2.ravel()\n",
    "\n",
    "    # If the min value is zero, consider only non-zero pixels\n",
    "    if np.min(ch1_flat) == 0 or np.min(ch2_flat) == 0:\n",
    "        mask = (ch1_flat > 0) & (ch2_flat > 0)\n",
    "        ch1_masked = ch1_flat[mask]\n",
    "        ch2_masked = ch2_flat[mask]\n",
    "    else:\n",
    "        ch1_masked = ch1_flat\n",
    "        ch2_masked = ch2_flat\n",
    "\n",
    "    if len(ch1_masked) == 0 or len(ch2_masked) == 0:\n",
    "        return 0, 0, 0, 0\n",
    "\n",
    "    # Center the data\n",
    "    ch1_mean = np.mean(ch1_masked)\n",
    "    ch2_mean = np.mean(ch2_masked)\n",
    "    ch1_centered = ch1_masked - ch1_mean\n",
    "    ch2_centered = ch2_masked - ch2_mean\n",
    "\n",
    "    # Perform PCA to find the orthogonal regression line\n",
    "    data = np.vstack([ch1_centered, ch2_centered]).T\n",
    "    cov_matrix = np.cov(data.T)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "    # The first principal component is the eigenvector with largest eigenvalue\n",
    "    principal_component = eigenvectors[:, np.argmax(eigenvalues)]\n",
    "\n",
    "    # Normalize the principal component\n",
    "    pc_norm = principal_component / np.linalg.norm(principal_component)\n",
    "\n",
    "    # Project all points onto the principal component line\n",
    "    projections = ch1_centered * pc_norm[0] + ch2_centered * pc_norm[1]\n",
    "\n",
    "    # Generate threshold parameters along the line\n",
    "    proj_min, proj_max = np.min(projections), np.max(projections)\n",
    "    t_values = np.linspace(proj_max, proj_min, num_thresholds)\n",
    "\n",
    "    best_thr_ch1 = best_thr_ch2 = 0\n",
    "    best_correlation = 1.0\n",
    "\n",
    "    for t in t_values:\n",
    "        # Convert parameter t back to (ch1, ch2) coordinates\n",
    "        thr_ch1 = ch1_mean + t * pc_norm[0]\n",
    "        thr_ch2 = ch2_mean + t * pc_norm[1]\n",
    "\n",
    "        # Skip if threshold is outside data range\n",
    "        if (\n",
    "            thr_ch1 < np.min(ch1_masked)\n",
    "            or thr_ch1 > np.max(ch1_masked)\n",
    "            or thr_ch2 < np.min(ch2_masked)\n",
    "            or thr_ch2 > np.max(ch2_masked)\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        # Create mask for pixels below thresholds\n",
    "        below_mask = (ch1_masked < thr_ch1) & (ch2_masked < thr_ch2)\n",
    "\n",
    "        if np.sum(below_mask) < 10:  # Need minimum number of pixels\n",
    "            continue\n",
    "\n",
    "        # Calculate correlation for pixels below threshold\n",
    "        ch1_below = ch1_masked[below_mask]\n",
    "        ch2_below = ch2_masked[below_mask]\n",
    "\n",
    "        if len(ch1_below) > 1 and np.std(ch1_below) > 0 and np.std(ch2_below) > 0:\n",
    "            correlation, _ = pearsonr(ch1_below, ch2_below)\n",
    "\n",
    "            # Find threshold where correlation is closest to zero\n",
    "            if abs(correlation) < abs(best_correlation):\n",
    "                best_correlation = correlation\n",
    "                best_thr_ch1 = thr_ch1\n",
    "                best_thr_ch2 = thr_ch2\n",
    "\n",
    "    # Calculate slope and intercept for reporting\n",
    "    slope = pc_norm[1] / pc_norm[0]\n",
    "    intercept = ch2_mean - slope * ch1_mean\n",
    "\n",
    "    return best_thr_ch1, best_thr_ch2, slope, intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f792a0a6",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c817765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "122d27c6",
   "metadata": {},
   "source": [
    "**Bonus:** Plot also a scatter plot of the two channels with the linear regression line.\n",
    "\n",
    "Note that the `costes_auto_threshold` function returns (in order) *Costes thresholds for channel 1*, *Costes thresholds for channel 2*, *slope* and *intercept* of the linear regression. We can use the slope and intercept to plot the linear regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc34d97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8604302",
   "metadata": {},
   "source": [
    "Now that we have the Costes thresholds, we can calculate the Mander's Correlation Coefficients using the Costes thresholds as we did for the Otsu thresholds.\n",
    "\n",
    "How do thresholds and mask images compare to the Otsu thresholds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85b35b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87520e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdccc9b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64bf50d3",
   "metadata": {},
   "source": [
    "### <mark style=\"color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;\">Image Rotation Test</mark>\n",
    "\n",
    "The **image rotation**, in this context, is a statistical method to validate colocalization significance. This method applies **rotations (90°, 180°, 270°) and flips (horizontal and vertical)** to one channel relative to the other, then recalculates the Mander's coefficients.\n",
    "\n",
    "**Note for Non-Square Images:** When working with non-square images, rotations by 90° and 270° would change the image dimensions (e.g., a 500×512 image becomes 512×500), making direct comparison impossible. To handle this, the function automatically pads non-square images to square dimensions with zeros before applying rotations, ensuring that all transformations maintain the same image size and allow valid statistical comparisons.\n",
    "\n",
    "Below you can find an implementation of this method in Python. This function returns the Mander's coefficients, the rotated/flipped Mander's coefficients, and the p-values for both coefficients.\n",
    "\n",
    "A low `p-value` (e.g. 0.0001) means that none of the rotations/flips produced M1/M2 values as high as the observed values without translation, indicating that the observed colocalization is statistically significant: the probability of getting the observed colocalization by random chance is < 0.0001 (less than 0.01%).\n",
    "\n",
    "Let's run it on the two channels we have been working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204b8f3f",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def image_rotation_test(\n",
    "    channel_1: np.ndarray,\n",
    "    channel_2: np.ndarray,\n",
    "    threshold_ch1: float = 0.0,\n",
    "    threshold_ch2: float = 0.0,\n",
    ") -> tuple[float, float, list[float], list[float], float, float]:\n",
    "    \"\"\"\n",
    "    Perform image rotation randomization test for Manders' coefficients validation.\n",
    "\n",
    "    This method applies systematic rotations (90°, 180°, 270°) and flips to one channel\n",
    "    relative to the other, breaking spatial relationships while preserving local patterns.\n",
    "\n",
    "    For non-square images, the function automatically pads them to square dimensions\n",
    "    with zeros before applying rotations to ensure valid comparisons.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    channel_1 : np.ndarray\n",
    "        First fluorescent channel (kept fixed)\n",
    "    channel_2 : np.ndarray\n",
    "        Second fluorescent channel (will be rotated and flipped)\n",
    "    threshold_ch1 : float, optional\n",
    "        Intensity threshold for channel 1\n",
    "    threshold_ch2 : float, optional\n",
    "        Intensity threshold for channel 2\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple containing:\n",
    "        - float: Observed M1 coefficient\n",
    "        - float: Observed M2 coefficient\n",
    "        - List[float]: M1 coefficients from rotation/flip iterations\n",
    "        - List[float]: M2 coefficients from rotation/flip iterations\n",
    "        - float: P-value for M1 (fraction of rotation M1 >= observed M1)\n",
    "        - float: P-value for M2 (fraction of rotation M2 >= observed M2)\n",
    "    \"\"\"\n",
    "\n",
    "    def _pad_to_square(image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Pad image to square dimensions with zeros.\"\"\"\n",
    "        h, w = image.shape\n",
    "        max_dim = max(h, w)\n",
    "        # Calculate padding needed\n",
    "        pad_h = (max_dim - h) // 2\n",
    "        pad_w = (max_dim - w) // 2\n",
    "        # Pad the image symmetrically\n",
    "        padded = np.pad(\n",
    "            image,\n",
    "            ((pad_h, max_dim - h - pad_h), (pad_w, max_dim - w - pad_w)),\n",
    "            mode=\"constant\",\n",
    "            constant_values=0,\n",
    "        )\n",
    "\n",
    "        return padded\n",
    "\n",
    "    def _calculate_manders(\n",
    "        ch1: np.ndarray, ch2: np.ndarray, thresh_ch1: float, thresh_ch2: float\n",
    "    ) -> tuple[float, float]:\n",
    "        \"\"\"Helper function to calculate Manders' correlation coefficients.\"\"\"\n",
    "        # Apply thresholds and get overlap mask\n",
    "        mask_a = ch1 > thresh_ch1\n",
    "        mask_b = ch2 > thresh_ch2\n",
    "        overlap_mask = mask_a & mask_b\n",
    "\n",
    "        # Calculate M1: fraction of A overlapping with B\n",
    "        m1_numerator = np.sum(ch1[overlap_mask])\n",
    "        m1_denominator = np.sum(ch1[mask_a])\n",
    "        m1 = m1_numerator / m1_denominator if m1_denominator > 0 else 0.0\n",
    "\n",
    "        # Calculate M2: fraction of B overlapping with A\n",
    "        m2_numerator = np.sum(ch2[overlap_mask])\n",
    "        m2_denominator = np.sum(ch2[mask_b])\n",
    "        m2 = m2_numerator / m2_denominator if m2_denominator > 0 else 0.0\n",
    "\n",
    "        return m1, m2\n",
    "\n",
    "    def _rotate_and_flip_image(\n",
    "        image: np.ndarray, rotation: int, flip_type: str\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Rotate image by specified angle and optionally flip.\"\"\"\n",
    "        # Apply rotation (k=1 means 90°, k=2 means 180°, k=3 means 270°)\n",
    "        rotated = np.rot90(image, k=rotation)\n",
    "\n",
    "        # Apply flip if requested\n",
    "        if flip_type == \"horizontal\":\n",
    "            rotated = np.fliplr(rotated)\n",
    "        elif flip_type == \"vertical\":\n",
    "            rotated = np.flipud(rotated)\n",
    "\n",
    "        return rotated\n",
    "\n",
    "    # Check if images are square, if not pad them\n",
    "    if (\n",
    "        channel_1.shape[0] != channel_1.shape[1]\n",
    "        or channel_2.shape[0] != channel_2.shape[1]\n",
    "    ):\n",
    "        # Pad both channels to square dimensions\n",
    "        channel_1_padded = _pad_to_square(channel_1)\n",
    "        channel_2_padded = _pad_to_square(channel_2)\n",
    "    else:\n",
    "        channel_1_padded = channel_1\n",
    "        channel_2_padded = channel_2\n",
    "\n",
    "    # Calculate observed Manders' coefficients\n",
    "    observed_m1, observed_m2 = _calculate_manders(\n",
    "        channel_1_padded, channel_2_padded, threshold_ch1, threshold_ch2\n",
    "    )\n",
    "\n",
    "    # Initialize lists to store rotation/flip coefficients\n",
    "    rotation_m1_values = []\n",
    "    rotation_m2_values = []\n",
    "\n",
    "    # Apply all combinations of rotations and flips\n",
    "    transformations = [\n",
    "        (1, None),  # 90° rotation\n",
    "        (1, \"horizontal\"),  # 90° rotation + horizontal flip\n",
    "        (1, \"vertical\"),  # 90° rotation + vertical flip\n",
    "        (2, None),  # 180° rotation\n",
    "        (2, \"horizontal\"),  # 180° rotation + horizontal flip\n",
    "        (2, \"vertical\"),  # 180° rotation + vertical flip\n",
    "        (3, None),  # 270° rotation\n",
    "        (3, \"horizontal\"),  # 270° rotation + horizontal flip\n",
    "        (3, \"vertical\"),  # 270° rotation + vertical flip\n",
    "    ]\n",
    "\n",
    "    for rotation, flip_type in transformations:\n",
    "        # Apply rotation and flip to channel 2 (using padded version)\n",
    "        transformed_ch2 = _rotate_and_flip_image(channel_2_padded, rotation, flip_type)\n",
    "\n",
    "        # Calculate Manders' coefficients with transformed channel 2\n",
    "        rotation_m1, rotation_m2 = _calculate_manders(\n",
    "            channel_1_padded, transformed_ch2, threshold_ch1, threshold_ch2\n",
    "        )\n",
    "        rotation_m1_values.append(rotation_m1)\n",
    "        rotation_m2_values.append(rotation_m2)\n",
    "\n",
    "    # Calculate p-values\n",
    "    n_transformations = len(rotation_m1_values)\n",
    "    p_value_m1 = np.sum(np.array(rotation_m1_values) >= observed_m1) / n_transformations\n",
    "    p_value_m2 = np.sum(np.array(rotation_m2_values) >= observed_m2) / n_transformations\n",
    "\n",
    "    return (\n",
    "        observed_m1,\n",
    "        observed_m2,\n",
    "        rotation_m1_values,\n",
    "        rotation_m2_values,\n",
    "        p_value_m1,\n",
    "        p_value_m2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7b1beb",
   "metadata": {},
   "source": [
    "Calculate either the Otsu or Costes thresholds for the two channels and then run the image translation randomization test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b2b9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f6c9f19",
   "metadata": {},
   "source": [
    "We can now print the results of the analysis: M1, M2, and their respective p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc06ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "473fc032",
   "metadata": {},
   "source": [
    "**Bonus:** We can also visualize the distribution of the random M1 and M2 values using a bar plot.\n",
    "\n",
    "To do this is useful to know that the transformations applied in `image_rotation_test` are 9:\n",
    "- 90°\n",
    "- 90° + Flip horizontally\n",
    "- 90° + Flip vertically\n",
    "- 180°\n",
    "- 180° + Flip horizontally\n",
    "- 180° + Flip vertically\n",
    "- 270°\n",
    "- 270° + Flip horizontally\n",
    "- 270° + Flip vertically\n",
    "\n",
    "We can use this information to plot the bar plot with the M1 and M2 values for each transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe69fb93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28be7657",
   "metadata": {},
   "source": [
    "### <mark style=\"color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;\">Image Translation Randomization Test</mark>\n",
    "\n",
    "The **image translation randomization** test is a statistical method used to **validate the significance of colocalization results**, particularly for Mander's coefficients. This method involves **randomly translating one channel relative to another and recalculating the Mander's coefficients** to create a distribution of values under the null hypothesis of no colocalization.\n",
    "\n",
    "Below you can find an implementation of this method in Python. This function returns the Mander's coefficients, the random Mander's coefficients, and the p-values for both coefficients.\n",
    "\n",
    "A low `p-value` (e.g. 0.0001) means that none of the `n` random translations (by default 1000) produced M1/M2 values as high as the observed values without translation, indicating that the observed colocalization is statistically significant: the probability of getting the observed colocalization by random chance is < 0.0001 (less than 0.01%).\n",
    "\n",
    "Let's run it on the two channels we have been working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0797d",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def image_translation_randomization(\n",
    "    channel_1: np.ndarray,\n",
    "    channel_2: np.ndarray,\n",
    "    threshold_ch1: float = 0.0,\n",
    "    threshold_ch2: float = 0.0,\n",
    "    n_iterations: int = 1000,\n",
    "    max_shift_fraction: float = 0.5,\n",
    "    seed: int = 3,\n",
    ") -> tuple[float, float, list[float], list[float], float, float]:\n",
    "    \"\"\"\n",
    "    Perform image translation randomization test for Manders' coefficients validation.\n",
    "\n",
    "    This method applies random translations (shifts) to one channel relative to the other,\n",
    "    breaking spatial relationships while preserving intensity distributions and local patterns.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    channel_1 : np.ndarray\n",
    "        First fluorescent channel (kept fixed)\n",
    "    channel_2 : np.ndarray\n",
    "        Second fluorescent channel (will be translated)\n",
    "    threshold_ch1 : float, optional\n",
    "        Intensity threshold for channel A (if None, uses Otsu's method)\n",
    "    threshold_ch2 : float, optional\n",
    "        Intensity threshold for channel B (if None, uses Otsu's method)\n",
    "    n_iterations : int\n",
    "        Number of randomization iterations (default: 1000)\n",
    "    max_shift_fraction : float\n",
    "        Maximum shift as fraction of image dimensions (default: 0.5)\n",
    "    seed : int\n",
    "        Random numpy seed for reproducibility (default: 3)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple containing:\n",
    "        - List[float]: M1 coefficients from randomized iterations (A overlap with B)\n",
    "        - List[float]: M2 coefficients from randomized iterations (B overlap with A)\n",
    "        - float: Observed M1 coefficient\n",
    "        - float: Observed M2 coefficient\n",
    "        - float: P-value for M1 (fraction of random M1 >= observed M1)\n",
    "        - float: P-value for M2 (fraction of random M2 >= observed M2)\n",
    "    \"\"\"\n",
    "\n",
    "    # Set numpy random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    def _calculate_manders(\n",
    "        ch1: np.ndarray, ch2: np.ndarray, thresh_ch1: float, thresh_ch2: float\n",
    "    ) -> tuple[float, float]:\n",
    "        \"\"\"Helper function to calculate Manders' correlations coefficients.\"\"\"\n",
    "        # Apply thresholds and get overlap mask\n",
    "        mask_a = ch1 > thresh_ch1\n",
    "        mask_b = ch2 > thresh_ch2\n",
    "        overlap_mask = mask_a & mask_b\n",
    "\n",
    "        # Calculate M1: fraction of A overlapping with B\n",
    "        m1_numerator = np.sum(ch1[overlap_mask])\n",
    "        m1_denominator = np.sum(ch1[mask_a])\n",
    "        m1 = m1_numerator / m1_denominator if m1_denominator > 0 else 0.0\n",
    "\n",
    "        # Calculate M2: fraction of B overlapping with A\n",
    "        m2_numerator = np.sum(ch2[overlap_mask])\n",
    "        m2_denominator = np.sum(ch2[mask_b])\n",
    "        m2 = m2_numerator / m2_denominator if m2_denominator > 0 else 0.0\n",
    "\n",
    "        return m1, m2\n",
    "\n",
    "    def _translate_image(image: np.ndarray, shift_y: int, shift_x: int) -> np.ndarray:\n",
    "        \"\"\"Translate image by given shifts with wrap-around.\"\"\"\n",
    "        return np.roll(np.roll(image, shift_y, axis=0), shift_x, axis=1)\n",
    "\n",
    "    # Calculate observed Manders' coefficients\n",
    "    observed_m1, observed_m2 = _calculate_manders(\n",
    "        channel_1, channel_2, threshold_ch1, threshold_ch2\n",
    "    )\n",
    "\n",
    "    # Calculate maximum shifts\n",
    "    max_shift_y = int(channel_2.shape[0] * max_shift_fraction)\n",
    "    max_shift_x = int(channel_2.shape[1] * max_shift_fraction)\n",
    "\n",
    "    # Initialize lists to store randomized coefficients\n",
    "    random_m1_values = []\n",
    "    random_m2_values = []\n",
    "\n",
    "    # for _ in tqdm(range(n_iterations), desc=\"Image translation randomization\"):\n",
    "    for _ in range(n_iterations):\n",
    "        # Generate random shifts (excluding zero shift)\n",
    "        shift_y = np.random.randint(-max_shift_y, max_shift_y + 1)\n",
    "        shift_x = np.random.randint(-max_shift_x, max_shift_x + 1)\n",
    "\n",
    "        # Ensure at least one shift is non-zero\n",
    "        if shift_y == 0 and shift_x == 0:\n",
    "            shift_y = np.random.choice([-1, 1])\n",
    "\n",
    "        # Apply translation to channel B\n",
    "        translated_ch2 = _translate_image(channel_2, shift_y, shift_x)\n",
    "\n",
    "        # Calculate Manders' coefficients with translated channel B\n",
    "        random_m1, random_m2 = _calculate_manders(\n",
    "            channel_1, translated_ch2, threshold_ch1, threshold_ch2\n",
    "        )\n",
    "        random_m1_values.append(random_m1)\n",
    "        random_m2_values.append(random_m2)\n",
    "\n",
    "    # Calculate p-values\n",
    "    p_value_m1 = np.sum(np.array(random_m1_values) >= observed_m1) / n_iterations\n",
    "    p_value_m2 = np.sum(np.array(random_m2_values) >= observed_m2) / n_iterations\n",
    "\n",
    "    return (\n",
    "        observed_m1,\n",
    "        observed_m2,\n",
    "        random_m1_values,\n",
    "        random_m2_values,\n",
    "        p_value_m1,\n",
    "        p_value_m2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa84e570",
   "metadata": {},
   "source": [
    "Calculate either the Otsu or Costes thresholds for the two channels and then run the image translation randomization test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169660a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48dc1dd3",
   "metadata": {},
   "source": [
    "We can now print the results of the analysis: M1, M2, and their respective p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c24fa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "744c4eb5",
   "metadata": {},
   "source": [
    "**Bonus:** We can also visualize the distribution of the random M1 and M2 values using histograms. This will help us understand the significance of our observed values in the context of the random distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb4864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfc21c1a",
   "metadata": {},
   "source": [
    "### <mark style=\"color: black; background-color: rgb(190,223,185); padding: 3px; border-radius: 5px;\">Summary</mark>\n",
    "\n",
    "The Python implementation for calculating Mander's Correlation Coefficients is straightforward and concise, as demonstrated in the code below.\n",
    "\n",
    "```python\n",
    "# Create binary mask for channel 1 & 2 using a thresholding method of choice\n",
    "threshold_ch1, threshold_ch2 = threshold_method(ch1, ch2)\n",
    "image_1_mask = ch1 > threshold_ch1\n",
    "image_2_mask = ch2 > threshold_ch2\n",
    "\n",
    "# Find pixels that are above threshold in both channels\n",
    "overlap_mask = image_1_mask & image_2_mask\n",
    "\n",
    "# Extract channel 1 & 2 intensities only from overlapping regions\n",
    "ch1_coloc = ch1[overlap_mask]\n",
    "ch2_coloc = ch2[overlap_mask]\n",
    "\n",
    "# Extract all channel 1 & 2 intensities above threshold\n",
    "ch1_tr = ch1[image_1_mask]\n",
    "ch2_tr = ch2[image_2_mask]\n",
    "\n",
    "# Calculate total intensity of channel 1 & 2 above threshold\n",
    "sum_ch1_tr = np.sum(ch1_tr)\n",
    "sum_ch2_tr = np.sum(ch2_tr)\n",
    "\n",
    "# M1: fraction of channel 1 intensity that colocalizes with channel 2\n",
    "M1 = np.sum(ch1_coloc) / sum_ch1_tr\n",
    "# M2: fraction of channel 2 intensity that colocalizes with channel 1\n",
    "M2 = np.sum(ch2_coloc) / sum_ch2_tr\n",
    "```\n",
    "\n",
    "**Key Considerations for Mander's Correlation Analysis:**\n",
    "\n",
    "1. **Threshold Sensitivity**: Mander's coefficients are **highly sensitive to thresholding methods**. The choice of thresholding strategy will significantly impact your results, making careful threshold selection essential for accurate colocalization analysis. Consider using:\n",
    "   - **Automated thresholding methods** (like Costes auto-threshold): these are fully automated and don't require you to select or tune any parameters\n",
    "   - **Threshold calculation algorithms** (like Otsu, Li, Triangle, Yen, etc.): these automatically calculate a threshold value, but you need to choose which algorithm to use based on your image characteristics and how the resulting threshold looks\n",
    "   - **Consistent thresholding**: use the same thresholding approach across experimental conditions\n",
    "\n",
    "2. **Background Considerations**: sometimes it is necessary to apply appropriate image preprocessing steps before calculating Mander's coefficients such as Background subtraction to remove non-specific signal or Flat-field correction to account for illumination variations.\n",
    "\n",
    "3. **Statistical Validation**: always validate your results using statistical tests such as the image rotation test or the image translation randomization test demonstrated above. This helps assess whether observed colocalization is statistically significant or could have occurred by chance.\n",
    "\n",
    "4. **Comparative Analysis**: Mander's coefficients should not be interpreted as absolute values in isolation. Instead, consider them in the context of:\n",
    "   - Comparisons between different experimental conditions\n",
    "   - Control vs. treatment groups\n",
    "   - Different time points or developmental stages\n",
    "   - Relative changes between conditions are often more meaningful than absolute values\n",
    "\n",
    "5. **Asymmetry Interpretation**: remember that M1 ≠ M2 is common and biologically meaningful.\n",
    "   - **M1**: Fraction of channel 1 intensity that overlaps with channel 2\n",
    "   - **M2**: Fraction of channel 2 intensity that overlaps with channel 1\n",
    "   - This asymmetry can reveal important biological relationships between the labeled structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0c7c9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bobiac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
